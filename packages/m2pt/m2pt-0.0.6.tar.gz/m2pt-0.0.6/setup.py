# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['m2pt']

package_data = \
{'': ['*']}

install_requires = \
['einops', 'torch', 'zetascale']

setup_kwargs = {
    'name': 'm2pt',
    'version': '0.0.6',
    'description': 'M2PT - Pytorch',
    'long_description': '[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n\n# Multi-Modal Pathway Transformer\n\n![Diagram](diagram.png)\n\nImplementation of M2PT in PyTorch from the paper: "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities".  [PAPER LINK](https://arxiv.org/abs/2401.14405). This is really really cool because just by merging the projections of different multi-modal models together you can increase the performance of your base model. This is a small but effective technique that can be implemented in any model with a minor plug in.\n\n\n## Install\n`pip3 install -U m2pt`\n\n## Usage\n\n### `M2PT`\nA fully ready to train implementation of the M2PT model that can be merged with the linears from any multi-modal models, just plug it in! It takes in tokenized texts which are integers then embeds them and then passes -> them into the transformer blocks and then at the end projects them and applies a softmax\n\n```python\nimport torch\nfrom torch import nn\nfrom m2pt.main import M2PT\n\n# Create an instance of the M2PT model class with the specified parameters\nmodel = M2PT(\n    dim=512,  # Dimension of the input and output tensors\n    num_tokens=10000,\n    depth=6,\n    dim_head=64,  # Dimension of each attention head\n    heads=8,  # Number of attention heads\n    dropout=0.1,  # Dropout rate\n    ff_mult=4,  # Multiplier for the dimension of the feed-forward network\n    original_linear=nn.Linear(512, 512),  # Linear layer for the original input tensor\n    auxiliar_linear=nn.Linear(512, 512),  # Linear layer for the auxiliary input tensor\n    ffn_original_linear=nn.Linear,  # Linear layer for the original input tensor in the feed-forward network\n    ffn_auxiliar_linear=nn.Linear,  # Linear layer for the auxiliary input tensor in the feed-forward network\n    ffn_original_last_linear=nn.Linear,  # Last linear layer for the original input tensor in the feed-forward network\n    ffn_aux_last_linear=nn.Linear,  # Last linear layer for the auxiliary input tensor in the feed-forward network\n)\n\n# Create a 3D tensor with shape B x S x D\nx = torch.randint(0, 10000, (1, 512))\n\n# Pass the input tensor through the model\nout = model(x)\n\n# Print the shape of the output tensor\nprint(out.shape)\n```\n\n\n\n### `MPTransformerBlock`\n\n- Implementation of Figure 2 and the Multimodal Pathway Transformer with cross modal FFN, plug in and play your FFN\n\n- Re-Usable and Modular.\n\n- Combines linear projections from multiple models\n\n\n```python\nimport torch\nfrom torch import nn\nfrom m2pt import MPTransformerBlock\n\n# Create an instance of the MPTransformerBlock class with the specified parameters\nmodel = MPTransformerBlock(\n    dim=512,  # Dimension of the input and output tensors\n    dim_head=64,  # Dimension of each attention head\n    heads=8,  # Number of attention heads\n    dropout=0.1,  # Dropout rate\n    ff_mult=4,  # Multiplier for the dimension of the feed-forward network\n    original_linear=nn.Linear(512, 512),  # Linear layer for the original input tensor\n    auxiliar_linear=nn.Linear(512, 512),  # Linear layer for the auxiliary input tensor\n    ffn_original_linear=nn.Linear,  # Linear layer for the original input tensor in the feed-forward network\n    ffn_auxiliar_linear=nn.Linear,  # Linear layer for the auxiliary input tensor in the feed-forward network\n    ffn_original_last_linear=nn.Linear,  # Last linear layer for the original input tensor in the feed-forward network\n    ffn_aux_last_linear=nn.Linear,  # Last linear layer for the auxiliary input tensor in the feed-forward network\n)\n\n# Create a 3D tensor with shape B x S x D\nx = torch.randn(1, 512, 512)\n\n# Pass the input tensor through the model\nout = model(x)\n\n# Print the shape of the output tensor\nprint(out.shape)\n\n\n```\n\n\n### `CrossModalReparameterization`\n- Implementation of the Cross Modal Reparameterization from the paper in Figure 2 and section 3.2\n\n- It combines the linear methods of different multi-modal models and kinda merges them through addition and a constant value lambda or Cross Modal Scale\n\n- Modular & Re-usable: Simply plug in your linears from any models!\n\n```python\nimport torch\n\nimport torch.nn as nn\n\nfrom transformers import BertModel, BertConfig, ViTModel, ViTConfig\n\nfrom m2pt import CrossModalReparameterization\n\n# Define a simple Transformer model for text\nclass TextTransformerModel(nn.Module):\n    def __init__(self, bert_model_name=\'bert-base-uncased\'):\n        super(TextTransformerModel, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model_name)\n\n        # Assume we\'re reparameterizing the first linear layer of the classifier\n        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        logits = self.classifier(pooled_output)\n        return logits\n\n# Define a simple Transformer model for images (using ViT for example)\nclass ImageTransformerModel(nn.Module):\n    def __init__(self, vit_model_name=\'google/vit-base-patch16-224\'):\n        super(ImageTransformerModel, self).__init__()\n        self.vit = ViTModel.from_pretrained(vit_model_name)\n\n        # Assume we\'re using the first linear layer of the classifier as the auxiliary layer\n        self.classifier = nn.Linear(self.vit.config.hidden_size, 2)\n\n    def forward(self, pixel_values):\n        outputs = self.vit(pixel_values=pixel_values)\n        pooled_output = outputs.pooler_output\n        logits = self.classifier(pooled_output)\n        return logits\n\n# Example usage\n# Initialize both models\ntext_model = TextTransformerModel()\nimage_model = ImageTransformerModel()\n\n# Assume we want to reparameterize the classifier layer of the text model\n# using the classifier layer of the image model\ncross_modal_layer = CrossModalReparameterization(text_model.classifier, image_model.classifier)\n\n# Replace the classifier in the text model with the cross-modal layer\ntext_model.classifier = cross_modal_layer\n\n# Example input (batch_size, sequence_length)\ninput_ids = torch.randint(0, 1000, (8, 512))\nattention_mask = torch.ones(8, 512)\n\n# Forward pass through the reparameterized model\nlogits = text_model(input_ids, attention_mask)\nprint(logits)\n\n# Train the text model as usual...\n\n# After training, merge the parameters for inference\ntext_model.classifier.merge_parameters()\n\n\n\n```\n\n\n## Citation\n```bibtex\n@misc{zhang2024multimodal,\n    title={Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities}, \n    author={Yiyuan Zhang and Xiaohan Ding and Kaixiong Gong and Yixiao Ge and Ying Shan and Xiangyu Yue},\n    year={2024},\n    eprint={2401.14405},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n\n# License\nMIT\n',
    'author': 'Kye Gomez',
    'author_email': 'kye@apac.ai',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'https://github.com/kyegomez/M2PT',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.6,<4.0',
}


setup(**setup_kwargs)
