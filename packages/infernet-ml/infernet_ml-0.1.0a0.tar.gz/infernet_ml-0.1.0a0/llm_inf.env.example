# the default workflow class to load. Change this to load a custom workflow
LLM_INF_WORKFLOW_CLASS="ml.workflows.inference.llm_inference_workflow.LLMInferenceWorkflow"

# first arg is tgi(Text Generation Interface) service url, second arg is connection timeout
LLM_INF_WORKFLOW_POSITIONAL_ARGS=["http://FILL_HOSTNAME_HERE", 30 ]

# Any arg passed here will be defaulted when sending to tgi
LLM_INF_WORKFLOW_KW_ARGS={}

# Parameters to control llm inference workflow retry logic

# number of retries
TGI_REQUEST_TRIES=3

# delays between retries in seconds
TGI_REQUEST_DELAY=3

# max delay between retry in seconds
TGI_REQUEST_MAX_DELAY=10

# backoff between retry in seconds
TGI_REQUEST_BACKOFF=2

# jitter to add to requests in seconds
TGI_REQUEST_JITTER=[0.5,1.5]
