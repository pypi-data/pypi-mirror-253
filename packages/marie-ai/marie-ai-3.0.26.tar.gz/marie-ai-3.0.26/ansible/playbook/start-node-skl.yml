---

- name: Deploy to node
  hosts: ml.cluster
  gather_facts: true
  vars:
    stack_name: "dev"

    # CORR
#    image_tag: 3.0.23-cuda
#    config_tag: marie-3.0.2x-corr.yml
#
    image_tag: 3.0.24-cpu
    config_tag: marie-3.0.2x-sk.yml

    server_port: 8080
  tasks:
#    - name: Create network
#      community.docker.docker_network:
#        name: "marieai"
#        state: present
#        driver: bridge
    # https://docs.ansible.com/ansible/latest/collections/community/docker/docker_container_module.html
    - name: Create volume for torch cache
      community.docker.docker_volume:
        name: torch_cache

    - name: Run the inference-server
      community.docker.docker_container:
        name: "marieai-{{ stack_name }}-server-sk"
        image: "marieai/marie:{{ image_tag }}"
        state: started
        init: true # Run an init inside the container that forwards signals and reaps processes
        recreate: "{{ (stack_name == 'dev') | bool }}"
#        auto_remove: "{{ (stack_name == 'dev') | bool }}"
        pull: true
        tty: true
        restart_policy: "always" # always | on-failure | unless-stopped
#        network_mode: marieai
        network_mode: host #"marie-{{ stack_name }}"
        command: server --start --uses /etc/marie/config/service/{{ config_tag }}
#        device_requests:
#          - # Add nVidia GPUs to this container
#            driver: nvidia
#            count: -1  # this means we want all
#            capabilities:
#              - gpu
#              - compute
#              - utility
#              # See https://github.com/NVIDIA/nvidia-container-runtime#supported-driver-capabilities
#              # for a list of capabilities supported by the nvidia driver

        log_driver: local
        log_options:
          max-size: 100m
          max-file: 10
        env:
#          CUDA_LAUNCH_BLOCKING: "1" # Debug CUDA
          MARIE_SKIP_MODEL_CACHE: "true"
          MARIE_DEFAULT_MOUNT: "/etc/marie"
          MARIE_LOG_CONFIG: docker
          MARIE_DEPLOYMENT_NAME: marie
          COLUMNS: "180"
          JINA_MP_START_METHOD: "fork"
          JINA_LOG_LEVEL:
            "{{ lookup('ansible.builtin.env', 'JINA_LOG_LEVEL') | default('DEBUG', true) }}"
#        ports:
#          - "{{ server_port }}:8080"
        volumes:
          - "/mnt/data/marie-ai/config:/etc/marie/config:ro"
          - "/mnt/data/marie-ai/model_zoo:/etc/marie/model_zoo:rw"
#          - "torch_cache:/etc/marie/model_zoo/cache/torch:rw"

# Docker debug command
# docker container ls -aq | xargs --no-run-if-empty docker stop && docker rm $(docker ps --filter status=exited -q)
# docker run --gpus all --name=marieai  --network=host -e JINA_LOG_LEVEL=debug -e MARIE_DEFAULT_MOUNT='/etc/marie' -v /mnt/data/marie-ai/config:/etc/marie/config:ro -v /mnt/data/marie-ai/model_zoo:/etc/marie/model_zoo:rw marieai/marie:3.0.22-cuda server --start --uses /etc/marie/config/service/marie-3.0.2x-corr.yml

# docker run --rm --gpus all --name=marieai  --network=host -e JINA_LOG_LEVEL=debug -e MARIE_DEFAULT_MOUNT='/etc/marie' -v /mnt/data/marie-ai/config:/etc/marie/config:ro -v /mnt/data/marie-ai/model_zoo:/etc/marie/model_zoo:rw marieai/marie:3.0.22-cuda server --start --uses /etc/marie/config/service/marie-3.0.2x-corr.yml
# Using BRIDGE NETWORK
#docker run --rm --gpus all --name=marieai  --network bridge -e JINA_LOG_LEVEL=debug -e MARIE_DEFAULT_MOUNT='/etc/marie' -v /mnt/data/marie-ai/config:/etc/marie/config:ro -v /mnt/data/marie-ai/model_zoo:/etc/marie/model_zoo:rw marieai/marie:3.0.23-cuda server --start --uses /etc/marie/config/service/marie-3.0.2x-corr.yml

# sudo systemctl stop  efs.service
# sudo systemctl stop  eraagent.service
#
